# tushare.py
import tushare as ts
import pandas as pd
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta
import os
import time
import json
from pathlib import Path
from src.data_sources.base import BaseStockDB


class TushareDB(BaseStockDB):
    def __init__(self, token: str, db_path: str = "data/tushare_data.db"):
        """
        åˆå§‹åŒ– Tushare æ•°æ®åº“

        Args:
            token: Tushare API token
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        """
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(db_path)

        # åˆå§‹åŒ– Tushare API
        ts.set_token(token)
        self.pro = ts.pro_api()

        # APIè°ƒç”¨é€Ÿç‡é™åˆ¶è¿½è¸ª
        self._api_call_times = []  # è®°å½•æœ€è¿‘APIè°ƒç”¨çš„æ—¶é—´
        self._rate_limit_delay = 1.3  # æ¯æ¬¡APIè°ƒç”¨çš„æœ€å°é—´éš”ï¼ˆç§’ï¼‰ï¼Œä¿å®ˆè®¾ç½®ä¸º1.3ç§’
        self._max_calls_per_minute = 50  # æ¯åˆ†é’Ÿæœ€å¤§è°ƒç”¨æ¬¡æ•°

    def _standardize_code(self, symbol: str) -> str:
        """
        æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç æ ¼å¼
        è¾“å…¥: 600382 æˆ– 600382.SH
        è¾“å‡º: 600382.SH
        """
        # Ensure symbol is a string
        if not isinstance(symbol, str):
            symbol = str(symbol)

        if '.' in symbol:
            return symbol.upper()

        # è‡ªåŠ¨åˆ¤æ–­äº¤æ˜“æ‰€
        if symbol.startswith(('600', '601', '603', '604', '605', '688', '689')):
            return f"{symbol}.SH"  # ä¸Šäº¤æ‰€
        elif symbol.startswith(('000', '001', '002', '003', '300', '301')):
            return f"{symbol}.SZ"  # æ·±äº¤æ‰€
        else:
            raise ValueError(f"æ— æ³•è¯†åˆ«è‚¡ç¥¨ä»£ç : {symbol}")

    def _detect_exchange(self, symbol: str) -> str:
        """è‡ªåŠ¨è¯†åˆ«äº¤æ˜“æ‰€"""
        # å¦‚æœåŒ…å«äº¤æ˜“æ‰€åç¼€ï¼Œç›´æ¥ä½¿ç”¨
        if '.' in symbol:
            suffix = symbol.split('.')[1].upper()
            if suffix == 'SH':
                return 'SSE'
            elif suffix == 'SZ':
                return 'SZSE'

        # å¦åˆ™æ ¹æ®ä»£ç å‰ç¼€åˆ¤æ–­
        code = symbol.split('.')[0] if '.' in symbol else symbol
        if code.startswith(('600', '601', '603', '604', '605', '688', '689')):
            return 'SSE'
        elif code.startswith(('000', '001', '002', '003', '300', '301')):
            return 'SZSE'
        else:
            return 'UNKNOWN'

    def _wait_for_rate_limit(self):
        """
        ç¡®ä¿ä¸è¶…è¿‡APIé¢‘ç‡é™åˆ¶ï¼ˆæ¯åˆ†é’Ÿ50æ¬¡ï¼‰

        è®¡ç®—é€»è¾‘ï¼š
        - å¦‚æœæœ€è¿‘50æ¬¡è°ƒç”¨éƒ½åœ¨1åˆ†é’Ÿå†…ï¼Œéœ€è¦ç­‰å¾…
        - æ¯æ¬¡è°ƒç”¨é—´éš”è‡³å°‘1.3ç§’ï¼ˆä¿å®ˆå€¼ï¼Œ60/50=1.2ç§’ï¼‰
        """
        if self._api_call_times:
            # è·å–æœ€åä¸€æ¬¡è°ƒç”¨æ—¶é—´
            last_call_time = self._api_call_times[-1]
            time_since_last_call = (datetime.now() - last_call_time).total_seconds()

            # å¦‚æœè·ç¦»ä¸Šæ¬¡è°ƒç”¨æ—¶é—´ä¸è¶³æœ€å°é—´éš”ï¼Œç­‰å¾…
            if time_since_last_call < self._rate_limit_delay:
                wait_time = self._rate_limit_delay - time_since_last_call
                time.sleep(wait_time)

        # è®°å½•æœ¬æ¬¡è°ƒç”¨æ—¶é—´
        self._api_call_times.append(datetime.now())

        # æ¸…ç†è¶…è¿‡1åˆ†é’Ÿçš„æ—§è®°å½•ï¼ˆä¿ç•™æœ€è¿‘1åˆ†é’Ÿçš„è®°å½•å³å¯ï¼‰
        one_minute_ago = datetime.now() - timedelta(minutes=1)
        self._api_call_times = [
            t for t in self._api_call_times if t > one_minute_ago
        ]

        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæœ€è¿‘1åˆ†é’Ÿå†…å·²ç»æœ‰50æ¬¡è°ƒç”¨ï¼Œç­‰å¾…åˆ°ä¸‹ä¸€æ¬¡å¯ç”¨æ—¶é—´
        if len(self._api_call_times) >= self._max_calls_per_minute:
            # ç­‰å¾…åˆ°æœ€æ—©çš„è°ƒç”¨æ—¶é—´è¶…è¿‡1åˆ†é’Ÿ
            oldest_call = self._api_call_times[0]
            wait_until = oldest_call + timedelta(minutes=1)
            wait_seconds = (wait_until - datetime.now()).total_seconds()
            if wait_seconds > 0:
                print(f"  â¸ï¸  APIé¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_seconds:.1f} ç§’...")
                time.sleep(wait_seconds)
                # æ¸…ç©ºè®°å½•ï¼Œé‡æ–°å¼€å§‹è®¡æ•°
                self._api_call_times = []

    def _retry_api_call(self, func, *args, max_retries=3, **kwargs):
        """
        å¸¦é‡è¯•æœºåˆ¶å’Œé€Ÿç‡é™åˆ¶çš„APIè°ƒç”¨

        Args:
            func: è¦è°ƒç”¨çš„å‡½æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            å‡½æ•°è¿”å›å€¼ï¼Œå¤±è´¥è¿”å›None
        """
        for attempt in range(max_retries):
            try:
                # ç­‰å¾…ä»¥æ»¡è¶³é€Ÿç‡é™åˆ¶
                self._wait_for_rate_limit()

                # æ‰§è¡ŒAPIè°ƒç”¨
                return func(*args, **kwargs)
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿ï¼š1ç§’ã€2ç§’ã€4ç§’
                    print(f"  âš ï¸  ç¬¬ {attempt + 1} æ¬¡è°ƒç”¨å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"  âŒ é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥")
                    return None

    def save_daily(self, symbol: str, start_date: str = "20200101",
                   end_date: str = None, adjust: str = None):
        """
        ä¿å­˜ A è‚¡æ—¥çº¿æ•°æ®ï¼ˆå…ˆæ£€æŸ¥æœ¬åœ°æ•°æ®åº“ï¼Œé¿å…é‡å¤è°ƒç”¨APIï¼‰

        Args:
            symbol: è‚¡ç¥¨ä»£ç ï¼Œå¯ä»¥æ˜¯ 600382 æˆ– 600382.SH
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDDï¼ŒNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼
            adjust: å¤æƒç±»å‹ï¼Œqfq=å‰å¤æƒ, hfq=åå¤æƒ, ''=ä¸å¤æƒã€‚Noneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼
        """
        # å¦‚æœæœªæŒ‡å®šå¤æƒç±»å‹ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–
        if adjust is None:
            from config.settings import DEFAULT_ADJUST
            adjust = DEFAULT_ADJUST

        # å¦‚æœæœªæŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
        if end_date is None:
            end_date = datetime.today().strftime("%Y%m%d")

        # ç¬¬é›¶æ­¥ï¼šä½¿ç”¨åŸºç±»æ–¹æ³•æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡ä¸‹è½½
        should_skip, reason = self.should_skip_download(symbol, start_date, end_date)
        if should_skip:
            print(f"â­ï¸  {symbol} {reason}")
            return
        else:
            print(f"ğŸ“¥ {symbol} {reason}ï¼Œå¼€å§‹ä¸‹è½½...")

        # æ ‡å‡†åŒ–ä»£ç 
        ts_code = self._standardize_code(symbol)

        # ç¬¬ä¸€æ­¥ï¼šè·å–æ•°æ®ï¼ˆå§‹ç»ˆè·å–ä¸å¤æƒæ•°æ® + å¤æƒå› å­ï¼‰
        try:
            # è·å–æ—¥çº¿æ•°æ®ï¼ˆä¸å¤æƒï¼Œè·å–æ‰€æœ‰å­—æ®µï¼‰- ä½¿ç”¨é‡è¯•æœºåˆ¶
            df = self._retry_api_call(
                self.pro.daily,
                ts_code=ts_code,
                start_date=start_date,
                end_date=end_date
            )

            if df is None or df.empty:
                print(f"âš ï¸ {symbol} æ— æ•°æ®")
                return

            # ä¿å­˜ä¸å¤æƒä»·æ ¼
            df['open_orig'] = df['open']
            df['high_orig'] = df['high']
            df['low_orig'] = df['low']
            df['close_orig'] = df['close']

            # è·å–å¤æƒå› å­å¹¶è®¡ç®—å‰å¤æƒä»·æ ¼
            try:
                adj_df = self.pro.adj_factor(ts_code=ts_code, start_date=start_date, end_date=end_date)
                df = df.merge(adj_df, on=['ts_code', 'trade_date'], how='left')

                # è®¡ç®—å‰å¤æƒä»·æ ¼
                df['open_qfq'] = df['open'] * df['adj_factor']
                df['high_qfq'] = df['high'] * df['adj_factor']
                df['low_qfq'] = df['low'] * df['adj_factor']
                df['close_qfq'] = df['close'] * df['adj_factor']
            except:
                # å¦‚æœè·å–å¤æƒå› å­å¤±è´¥ï¼Œå‰å¤æƒä»·æ ¼ä¸º None
                print(f"  âš ï¸  æ— æ³•è·å–å¤æƒå› å­ï¼Œå‰å¤æƒä»·æ ¼å°†ä¸ºç©º")
                df['open_qfq'] = None
                df['high_qfq'] = None
                df['low_qfq'] = None
                df['close_qfq'] = None

            # æ ¹æ®é…ç½®å†³å®šä½¿ç”¨å“ªç§ä»·æ ¼ä½œä¸ºä¸»ä»·æ ¼ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
            if adjust == 'qfq':
                df['open'] = df['open_qfq']
                df['high'] = df['high_qfq']
                df['low'] = df['low_qfq']
                df['close'] = df['close_qfq']
            elif adjust == 'hfq':
                # åå¤æƒ = å½“å‰ä»· / å¤æƒå› å­
                df['open'] = df['open'] / df['adj_factor']
                df['high'] = df['high'] / df['adj_factor']
                df['low'] = df['low'] / df['adj_factor']
                df['close'] = df['close'] / df['adj_factor']
            else:
                # ä¸å¤æƒï¼Œä½¿ç”¨åŸå§‹ä»·æ ¼
                df['open'] = df['open_orig']
                df['high'] = df['high_orig']
                df['low'] = df['low_orig']
                df['close'] = df['close_orig']

            # è·å–æ¯æ—¥åŸºæœ¬é¢æŒ‡æ ‡ï¼ˆdaily_basicï¼‰ï¼Œå¦‚æœæ— æƒé™åˆ™è·³è¿‡
            basic_data_available = False
            try:
                basic = self._retry_api_call(
                    self.pro.daily_basic,
                    ts_code=ts_code,
                    start_date=start_date,
                    end_date=end_date,
                    fields='ts_code,trade_date,turnover_rate,turnover_rate_f,volume_ratio,pe,pe_ttm,pb,ps,ps_ttm,dv_ratio,dv_ttm,total_share,float_share,free_share,total_mv,circ_mv'
                )

                if basic is not None and not basic.empty:
                    # åˆå¹¶æ‰€æœ‰ daily_basic å­—æ®µ
                    df = df.merge(basic, on=['ts_code', 'trade_date'], how='left')
                    basic_data_available = True
                    print(f"  âœ“ è·å–åˆ° daily_basic æ•°æ® {len(basic)} æ¡")
                else:
                    print(f"  âš ï¸  daily_basic æ•°æ®æš‚æœªç”Ÿæˆï¼ˆAPIæ›´æ–°å»¶è¿Ÿï¼‰ï¼Œç¨åå¯é‡è¯•æ›´æ–°æ¢æ‰‹ç‡")
                    # è®¾ç½®æ‰€æœ‰æ–°å­—æ®µä¸º None
                    for field in ['turnover_rate_f', 'volume_ratio', 'pe', 'pe_ttm', 'pb',
                                  'ps', 'ps_ttm', 'dv_ratio', 'dv_ttm', 'total_share',
                                  'float_share', 'free_share', 'total_mv', 'circ_mv']:
                        df[field] = None

            except Exception as e:
                # ä¼˜é›…å¤„ç†æƒé™é”™è¯¯
                if "æ— æƒé™" in str(e) or "æƒé™" in str(e) or "403" in str(e):
                    print(f"  âš ï¸  æ— æƒé™è·å– daily_basic æ•°æ®ï¼ˆéœ€è¦2000+ç§¯åˆ†ï¼‰")
                else:
                    print(f"  âš ï¸  è·å– daily_basic æ•°æ®å¤±è´¥: {e}")

                # è®¾ç½®æ‰€æœ‰å­—æ®µä¸º None
                for field in ['turnover', 'turnover_rate_f', 'volume_ratio', 'pe', 'pe_ttm', 'pb',
                              'ps', 'ps_ttm', 'dv_ratio', 'dv_ttm', 'total_share',
                              'float_share', 'free_share', 'total_mv', 'circ_mv']:
                    df[field] = None

            # é‡å‘½ååˆ—
            df = df.rename(columns={
                "trade_date": "datetime",
                "vol": "volume",
                "turnover_rate": "turnover"
            })

            # æ·»åŠ å…ƒæ•°æ®
            df["symbol"] = ts_code.split('.')[0]
            df["exchange"] = self._detect_exchange(ts_code)
            df["interval"] = "1d"
            df["datetime"] = pd.to_datetime(df["datetime"]).dt.strftime("%Y-%m-%d")

            # æ·»åŠ  amount åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if 'amount' not in df.columns:
                df['amount'] = None

            # é€‰æ‹©è¦ä¿å­˜çš„åˆ—ï¼ˆåŒ…å«æ‰€æœ‰ Tushare daily å­—æ®µ + å‰å¤æƒä»·æ ¼ + daily_basic æŒ‡æ ‡ï¼‰
            # Tushare daily å­—æ®µï¼šts_code, trade_date, open, high, low, close, pre_close, change, pct_chg, vol, amount
            columns = ["symbol", "exchange", "interval", "datetime",
                      "open", "high", "low", "close",  # ä¸å¤æƒä»·æ ¼ï¼ˆä¸»ä»·æ ¼åˆ—ï¼‰
                      "open_qfq", "high_qfq", "low_qfq", "close_qfq",  # å‰å¤æƒä»·æ ¼
                      "pre_close", "change", "pct_chg",  # Tushare é¢å¤–å­—æ®µ
                      "volume", "turnover", "amount",
                      # Daily basic æŒ‡æ ‡
                      "turnover_rate_f", "volume_ratio",
                      "pe", "pe_ttm", "pb", "ps", "ps_ttm",
                      "total_mv", "circ_mv",
                      "total_share", "float_share", "free_share",
                      "dv_ratio", "dv_ttm"]

            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨ï¼ˆæŸäº›å­—æ®µå¯èƒ½åœ¨æ—§æ•°æ®ä¸­ä¸å­˜åœ¨ï¼‰
            for col in columns:
                if col not in df.columns:
                    df[col] = None

        except Exception as e:
            # æ•°æ®è·å–å¤±è´¥
            print(f"âŒ {symbol} ä¸‹è½½å¤±è´¥: {e}")
            return

        # ç¬¬äºŒæ­¥ï¼šä¿å­˜åˆ°æ•°æ®åº“
        try:
            df[columns].to_sql(
                "bars", self.engine, if_exists="append", index=False, method="multi"
            )
            print(f"âœ… å·²ä¿å­˜ {symbol} å…± {len(df)} æ¡è®°å½•")
        except Exception as e:
            # æ•°æ®åº“æ“ä½œå¤±è´¥ï¼ˆæ¯”å¦‚é‡å¤æ•°æ®ï¼‰ï¼Œä¸æ˜¾ç¤ºä¸º"ä¸‹è½½å¤±è´¥"
            if "UNIQUE constraint" in str(e) or "duplicate" in str(e).lower():
                # æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡
                print(f"â­ï¸  {symbol} æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
            else:
                # å…¶ä»–æ•°æ®åº“é”™è¯¯
                print(f"âš ï¸  {symbol} æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")

    def save_multiple_stocks(self, symbols: list, start_date: str = "20200101",
                            end_date: str = None, adjust: str = None):
        """
        æ‰¹é‡ä¿å­˜å¤šåªè‚¡ç¥¨æ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            adjust: å¤æƒç±»å‹ï¼ŒNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼
        """
        # å¦‚æœæœªæŒ‡å®šå¤æƒç±»å‹ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–
        if adjust is None:
            from config.settings import DEFAULT_ADJUST
            adjust = DEFAULT_ADJUST

        for symbol in symbols:
            self.save_daily(symbol, start_date, end_date, adjust)

    def update_turnover_only(self, symbols: list = None, start_date: str = None, end_date: str = None):
        """
        å•ç‹¬æ›´æ–°æ¢æ‰‹ç‡ç­‰åŸºæœ¬é¢æ•°æ®ï¼ˆç”¨äºè¡¥å……ä¹‹å‰æœªè·å–åˆ°çš„ daily_basic æ•°æ®ï¼‰

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ŒNone åˆ™æ›´æ–°å…¨éƒ¨
            start_date: å¼€å§‹æ—¥æœŸï¼ŒNone åˆ™ä½¿ç”¨æœ€è¿‘7å¤©
            end_date: ç»“æŸæ—¥æœŸï¼ŒNone åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ

        Returns:
            æ›´æ–°çš„è®°å½•æ•°
        """
        from datetime import timedelta
        import pandas as pd

        # å¦‚æœæœªæŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
        if end_date is None:
            end_date = datetime.today().strftime("%Y%m%d")

        # å¦‚æœæœªæŒ‡å®šå¼€å§‹æ—¥æœŸï¼Œä½¿ç”¨æœ€è¿‘7å¤©
        if start_date is None:
            start_date = (datetime.today() - timedelta(days=7)).strftime("%Y%m%d")

        # è·å–è‚¡ç¥¨åˆ—è¡¨
        if symbols is None:
            # ä»æ•°æ®åº“è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
            query = """
            SELECT DISTINCT symbol FROM bars
            WHERE interval = '1d'
            """
            with self.engine.connect() as conn:
                df = pd.read_sql_query(query, conn)
            symbols = df['symbol'].tolist() if not df.empty else []

        if not symbols:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è‚¡ç¥¨")
            return 0

        print(f"ğŸ“Š å¼€å§‹æ›´æ–°æ¢æ‰‹ç‡æ•°æ®ï¼ˆ{start_date} - {end_date}ï¼‰")
        print(f"ğŸ“‹ å…± {len(symbols)} åªè‚¡ç¥¨")

        updated_count = 0
        skipped_count = 0

        for symbol in symbols:
            try:
                ts_code = self._standardize_code(symbol)

                # è·å– daily_basic æ•°æ®
                basic = self._retry_api_call(
                    self.pro.daily_basic,
                    ts_code=ts_code,
                    start_date=start_date,
                    end_date=end_date,
                    fields='ts_code,trade_date,turnover_rate,turnover_rate_f,volume_ratio,pe,pe_ttm,pb,ps,ps_ttm,dv_ratio,dv_ttm,total_share,float_share,free_share,total_mv,circ_mv'
                )

                if basic is None or basic.empty:
                    skipped_count += 1
                    continue

                # å‡†å¤‡æ›´æ–°æ•°æ®
                basic = basic.rename(columns={"trade_date": "datetime"})
                basic["datetime"] = pd.to_datetime(basic["datetime"]).dt.strftime("%Y-%m-%d")
                basic["symbol"] = symbol
                basic["turnover"] = basic["turnover_rate"]

                # åªæ›´æ–° turnover ç­‰å­—æ®µä¸º NULL çš„è®°å½•
                # ä½¿ç”¨ SQL UPDATE è¯­å¥é€æ¡æ›´æ–°
                with self.engine.connect() as conn:
                    for _, row in basic.iterrows():
                        update_sql = """
                        UPDATE bars
                        SET turnover = :turnover,
                            turnover_rate_f = :turnover_rate_f,
                            volume_ratio = :volume_ratio,
                            pe = :pe,
                            pe_ttm = :pe_ttm,
                            pb = :pb,
                            ps = :ps,
                            ps_ttm = :ps_ttm,
                            total_mv = :total_mv,
                            circ_mv = :circ_mv,
                            total_share = :total_share,
                            float_share = :float_share,
                            free_share = :free_share,
                            dv_ratio = :dv_ratio,
                            dv_ttm = :dv_ttm
                        WHERE symbol = :symbol
                          AND datetime = :datetime
                          AND turnover IS NULL
                        """
                        result = conn.execute(
                            text(update_sql),
                            {
                                "turnover": row.get("turnover"),
                                "turnover_rate_f": row.get("turnover_rate_f"),
                                "volume_ratio": row.get("volume_ratio"),
                                "pe": row.get("pe"),
                                "pe_ttm": row.get("pe_ttm"),
                                "pb": row.get("pb"),
                                "ps": row.get("ps"),
                                "ps_ttm": row.get("ps_ttm"),
                                "total_mv": row.get("total_mv"),
                                "circ_mv": row.get("circ_mv"),
                                "total_share": row.get("total_share"),
                                "float_share": row.get("float_share"),
                                "free_share": row.get("free_share"),
                                "dv_ratio": row.get("dv_ratio"),
                                "dv_ttm": row.get("dv_ttm"),
                                "symbol": row["symbol"],
                                "datetime": row["datetime"]
                            }
                        )
                        if result.rowcount > 0:
                            updated_count += 1
                    conn.commit()

                print(f"  âœ“ {symbol} æ›´æ–°äº† {len(basic)} æ¡è®°å½•")

            except Exception as e:
                print(f"  âŒ {symbol} æ›´æ–°å¤±è´¥: {e}")

        print(f"\n{'='*60}")
        print(f"æ¢æ‰‹ç‡æ›´æ–°å®Œæˆ:")
        print(f"  æˆåŠŸ: {updated_count} æ¡è®°å½•")
        print(f"  è·³è¿‡: {skipped_count} åªè‚¡ç¥¨")
        print(f"{'='*60}")

        return updated_count

    def _get_stock_name_from_api(self, symbol: str) -> str:
        """
        ä» Tushare API è·å–è‚¡ç¥¨åç§°

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            è‚¡ç¥¨åç§°ï¼Œå¤±è´¥åˆ™è¿”å› None
        """
        try:
            ts_code = self._standardize_code(symbol)
            basic = self.pro.stock_basic(ts_code=ts_code, fields='ts_code,name')
            if not basic.empty:
                return basic['name'].values[0]
        except:
            pass
        return None

    def get_stock_list(self, exchange: str = None) -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨åˆ—è¡¨

        Args:
            exchange: äº¤æ˜“æ‰€ SSE/SZSEï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
        """
        try:
            df = self.pro.stock_list(exchange=exchange)
            return df
        except Exception as e:
            print(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return pd.DataFrame()

    def check_permissions(self):
        """æ£€æŸ¥å½“å‰ token çš„æƒé™å’Œç§¯åˆ†"""
        try:
            print("=" * 60)
            print("Tushare æ¥å£æƒé™æµ‹è¯•:")
            print("=" * 60)

            # æµ‹è¯•å„ä¸ªæ¥å£
            print("\næ¥å£æƒé™æµ‹è¯•:")

            # æµ‹è¯•è‚¡ç¥¨åˆ—è¡¨æ¥å£
            try:
                stocks = self.pro.stock_list(exchange='SSE')
                print(f"  âœ… stock_list - å¯ç”¨")
            except Exception as e:
                print(f"  âŒ stock_list - æ— æƒé™")

            # æµ‹è¯•æ—¥çº¿æ•°æ®æ¥å£
            try:
                df = self.pro.daily(ts_code='000001.SZ', start_date='20250101', end_date='20250102')
                print(f"  âœ… daily - å¯ç”¨")
            except Exception as e:
                print(f"  âŒ daily - æ— æƒé™")

            # æµ‹è¯•è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯æ¥å£ï¼ˆstock_basicï¼‰
            try:
                df = self.pro.stock_basic(exchange='', list_status='L', fields='ts_code,symbol,name', limit=1)
                print(f"  âœ… stock_basic - å¯ç”¨ï¼ˆè·å–è‚¡ç¥¨åç§°ï¼‰")
            except Exception as e:
                print(f"  âŒ stock_basic - æ— æƒé™ï¼ˆæ— æ³•è·å–è‚¡ç¥¨åç§°ï¼‰")

            # æµ‹è¯•æ—¥çº¿åŸºæœ¬ä¿¡æ¯æ¥å£
            try:
                df = self.pro.daily_basic(ts_code='000001.SZ', start_date='20250101', end_date='20250102')
                print(f"  âœ… daily_basic - å¯ç”¨")
            except Exception as e:
                print(f"  âŒ daily_basic - æ— æƒé™")

            # æµ‹è¯•å¤æƒå› å­æ¥å£
            try:
                df = self.pro.adj_factor(ts_code='000001.SZ', start_date='20250101', end_date='20250102')
                print(f"  âœ… adj_factor - å¯ç”¨")
            except Exception as e:
                print(f"  âŒ adj_factor - æ— æƒé™")

            print("\næç¤º:")
            print("  - å¦‚æœæ˜¾ç¤ºæ— æƒé™ï¼Œéœ€è¦å‡çº§ Tushare ç§¯åˆ†")
            print("  - æ—¥çº¿æ•°æ®é€šå¸¸éœ€è¦ 2000+ ç§¯åˆ†")
            print("  - è®¿é—® https://tushare.pro æŸ¥çœ‹ç§¯åˆ†è§„åˆ™")
            print("=" * 60)

        except Exception as e:
            print(f"âŒ è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}")

    def _save_checkpoint(self, checkpoint_path: str, data: dict):
        """
        ä¿å­˜ä¸‹è½½è¿›åº¦æ£€æŸ¥ç‚¹

        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            data: è¦ä¿å­˜çš„æ•°æ®
        """
        try:
            with open(checkpoint_path, 'w') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            print(f"  âš ï¸  ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def _load_checkpoint(self, checkpoint_path: str) -> dict:
        """
        åŠ è½½ä¸‹è½½è¿›åº¦æ£€æŸ¥ç‚¹

        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„

        Returns:
            æ£€æŸ¥ç‚¹æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›ç©ºå­—å…¸
        """
        try:
            if Path(checkpoint_path).exists():
                with open(checkpoint_path, 'r') as f:
                    return json.load(f)
        except Exception as e:
            print(f"  âš ï¸  åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return {}

    def save_all_stocks_by_code(self, start_date: str = "20240101",
                                end_date: str = None,
                                adjust: str = None,
                                checkpoint_path: str = None,
                                resume: bool = True):
        """
        æŒ‰è‚¡ç¥¨ä»£ç å¾ªç¯è·å–å…¨éƒ¨Aè‚¡æ•°æ®

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            adjust: å¤æƒç±»å‹
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # è®¾ç½®é»˜è®¤æ£€æŸ¥ç‚¹è·¯å¾„
        if checkpoint_path is None:
            try:
                from config.settings import CHECKPOINT_FILE
                checkpoint_path = str(CHECKPOINT_FILE)
            except ImportError:
                checkpoint_path = "data/download_checkpoint.json"

        # 1. è·å–å…¨éƒ¨Aè‚¡åˆ—è¡¨
        print("ğŸ“‹ æ­£åœ¨è·å–è‚¡ç¥¨åˆ—è¡¨...")
        try:
            stock_list = self._retry_api_call(
                self.pro.stock_basic,
                exchange='',
                list_status='L',  # åªè·å–ä¸Šå¸‚è‚¡ç¥¨
                fields='ts_code,name,area,industry,list_date'
            )
            if stock_list is None or stock_list.empty:
                print("âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
                return None
            all_stocks = stock_list['ts_code'].tolist()
            print(f"ğŸ“‹ å…± {len(all_stocks)} åªè‚¡ç¥¨")
        except Exception as e:
            print(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return None

        # 2. å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤
        start_index = 0
        stats = {'success': 0, 'failed': 0, 'skipped': 0, 'total': len(all_stocks)}

        if resume:
            checkpoint = self._load_checkpoint(checkpoint_path)
            if checkpoint:
                # éªŒè¯æ£€æŸ¥ç‚¹æ˜¯å¦åŒ¹é…å½“å‰ä¸‹è½½ä»»åŠ¡
                if (checkpoint.get('start_date') == start_date and
                    checkpoint.get('end_date') == end_date and
                    checkpoint.get('adjust') == adjust and
                    checkpoint.get('total') == len(all_stocks)):

                    last_index = checkpoint.get('last_index', 0)
                    start_index = last_index + 1
                    stats = checkpoint.get('stats', stats)
                    print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: ç¬¬ {start_index + 1} åªè‚¡ç¥¨å¼€å§‹")
                else:
                    print("âš ï¸  æ£€æŸ¥ç‚¹å‚æ•°ä¸åŒ¹é…ï¼Œä»å¤´å¼€å§‹ä¸‹è½½")
                    Path(checkpoint_path).unlink(missing_ok=True)

        # 3. éå†æ¯åªè‚¡ç¥¨
        for i in range(start_index, len(all_stocks)):
            ts_code = all_stocks[i]

            # å®šæœŸæ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯50åªè‚¡ç¥¨ï¼‰
            if (i + 1) % 50 == 1 or i == len(all_stocks) - 1:
                print(f"\n{'='*60}")
                print(f"è¿›åº¦: [{i + 1}/{stats['total']}]")
                print(f"æˆåŠŸ: {stats['success']} | å¤±è´¥: {stats['failed']} | è·³è¿‡: {stats['skipped']}")
                print(f"{'='*60}")

            try:
                # è°ƒç”¨ç°æœ‰çš„ save_daily æ–¹æ³•
                self.save_daily(ts_code, start_date, end_date, adjust)
                stats['success'] += 1
            except Exception as e:
                print(f"âŒ {ts_code} å¤„ç†å¤±è´¥: {e}")
                stats['failed'] += 1

            # æ¯10åªè‚¡ç¥¨ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            if (i + 1) % 10 == 0:
                checkpoint_data = {
                    'start_date': start_date,
                    'end_date': end_date,
                    'adjust': adjust,
                    'total': len(all_stocks),
                    'last_index': i,
                    'stats': stats,
                    'timestamp': datetime.now().isoformat()
                }
                self._save_checkpoint(checkpoint_path, checkpoint_data)

        # 4. åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆä¸‹è½½å®Œæˆï¼‰
        if Path(checkpoint_path).exists():
            Path(checkpoint_path).unlink()
            print("ğŸ—‘ï¸  å·²åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶")

        # 5. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"æ•°æ®ä¸‹è½½å®Œæˆ:")
        print(f"  æ€»è®¡: {stats['total']} åªè‚¡ç¥¨")
        print(f"  æˆåŠŸ: {stats['success']}")
        print(f"  å¤±è´¥: {stats['failed']}")
        print(f"  è·³è¿‡: {stats['skipped']}")
        print(f"{'='*60}")

        return stats

    def save_all_stocks_by_code_incremental(self, default_start_date: str = "20240101",
                                           end_date: str = None,
                                           adjust: str = None,
                                           checkpoint_path: str = None,
                                           resume: bool = True,
                                           stock_list: list = None):
        """
        æŒ‰è‚¡ç¥¨ä»£ç å¢é‡æ›´æ–°Aè‚¡æ•°æ®ï¼ˆåªä¸‹è½½æ¯åªè‚¡ç¥¨çš„æœ€æ–°ç¼ºå¤±æ•°æ®ï¼‰

        Args:
            default_start_date: é»˜è®¤å¼€å§‹æ—¥æœŸï¼ˆç”¨äºæ²¡æœ‰æ•°æ®çš„è‚¡ç¥¨ï¼‰
            end_date: ç»“æŸæ—¥æœŸ
            adjust: å¤æƒç±»å‹
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
            stock_list: æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ï¼ŒNoneåˆ™è·å–å…¨éƒ¨Aè‚¡

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # è®¾ç½®é»˜è®¤æ£€æŸ¥ç‚¹è·¯å¾„
        if checkpoint_path is None:
            try:
                from config.settings import CHECKPOINT_FILE
                checkpoint_path = str(CHECKPOINT_FILE)
            except ImportError:
                checkpoint_path = "data/download_checkpoint.json"

        # å¦‚æœæœªæŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
        if end_date is None:
            end_date = datetime.today().strftime("%Y%m%d")

        # 1. è·å–è‚¡ç¥¨åˆ—è¡¨
        if stock_list is None:
            # è·å–å…¨éƒ¨Aè‚¡åˆ—è¡¨
            print("ğŸ“‹ æ­£åœ¨è·å–è‚¡ç¥¨åˆ—è¡¨...")
            try:
                stock_list_df = self._retry_api_call(
                    self.pro.stock_basic,
                    exchange='',
                    list_status='L',  # åªè·å–ä¸Šå¸‚è‚¡ç¥¨
                    fields='ts_code,name,area,industry,list_date'
                )
                if stock_list_df is None or stock_list_df.empty:
                    print("âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
                    return None
                all_stocks = stock_list_df['ts_code'].tolist()
                print(f"ğŸ“‹ å…± {len(all_stocks)} åªè‚¡ç¥¨")
            except Exception as e:
                print(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
                return None
        else:
            # ä½¿ç”¨æŒ‡å®šçš„è‚¡ç¥¨åˆ—è¡¨
            all_stocks = []
            print(f"ğŸ“‹ ä½¿ç”¨æŒ‡å®šçš„è‚¡ç¥¨åˆ—è¡¨...")

            for code in stock_list:
                # æ ‡å‡†åŒ–ä»£ç æ ¼å¼
                try:
                    ts_code = self._standardize_code(code)
                    all_stocks.append(ts_code)
                except Exception as e:
                    print(f"  âš ï¸  è‚¡ç¥¨ä»£ç  {code} æ ¼å¼ä¸æ­£ç¡®: {e}")

            if not all_stocks:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç ")
                return None

            print(f"ğŸ“‹ å…± {len(all_stocks)} åªè‚¡ç¥¨")

        # 2. æ£€æŸ¥æ¯åªè‚¡ç¥¨çš„æœ€æ–°æ•°æ®æ—¥æœŸ
        import pandas as pd
        from datetime import timedelta

        incremental_dates = {}
        need_update_stocks = []
        no_data_stocks = []

        print("ğŸ” æ£€æŸ¥æœ¬åœ°æ•°æ®æœ€æ–°æ—¥æœŸ...")

        for ts_code in all_stocks:
            try:
                code = ts_code.split('.')[0]

                # æŸ¥è¯¢è¯¥è‚¡ç¥¨çš„æœ€æ–°æ•°æ®æ—¥æœŸ
                query = """
                SELECT datetime FROM bars
                WHERE symbol = :symbol AND interval = '1d'
                ORDER BY datetime DESC LIMIT 1
                """
                with self.engine.connect() as conn:
                    df = pd.read_sql_query(
                        query,
                        conn,
                        params={"symbol": code}
                    )

                if not df.empty:
                    latest_date = pd.to_datetime(df['datetime'].iloc[0])
                    end_date_dt = pd.to_datetime(end_date)

                    # å¦‚æœæœ€æ–°æ•°æ®å·²ç»æ˜¯ä»Šå¤©æˆ–ä¹‹åï¼Œè·³è¿‡
                    if latest_date >= end_date_dt:
                        continue

                    # ä»æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹
                    start_date_dt = latest_date + timedelta(days=1)
                    start_date = start_date_dt.strftime("%Y%m%d")

                    incremental_dates[ts_code] = start_date
                    need_update_stocks.append(ts_code)
                else:
                    # æ— æ•°æ®ï¼Œéœ€è¦ä»å¤´ä¸‹è½½
                    no_data_stocks.append(ts_code)
                    incremental_dates[ts_code] = default_start_date
                    need_update_stocks.append(ts_code)

            except Exception as e:
                print(f"  âš ï¸  {ts_code} æ£€æŸ¥å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ä¹Ÿæ·»åŠ åˆ°éœ€è¦æ›´æ–°çš„åˆ—è¡¨
                incremental_dates[ts_code] = default_start_date
                need_update_stocks.append(ts_code)

        print(f"âœ… æ£€æŸ¥å®Œæˆ: {len(need_update_stocks)} åªéœ€è¦æ›´æ–°, {len(all_stocks) - len(need_update_stocks)} åªå·²æ˜¯æœ€æ–°")

        if len(need_update_stocks) == 0:
            print("ğŸ‰ æ‰€æœ‰è‚¡ç¥¨æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°ï¼")
            return {
                'total': len(all_stocks),
                'success': 0,
                'failed': 0,
                'skipped': len(all_stocks)
            }

        # 3. å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤
        start_index = 0
        stats = {'success': 0, 'failed': 0, 'skipped': len(all_stocks) - len(need_update_stocks), 'total': len(all_stocks)}

        if resume:
            checkpoint = self._load_checkpoint(checkpoint_path)
            if checkpoint:
                # éªŒè¯æ£€æŸ¥ç‚¹æ˜¯å¦åŒ¹é…å½“å‰ä¸‹è½½ä»»åŠ¡
                if (checkpoint.get('default_start_date') == default_start_date and
                    checkpoint.get('end_date') == end_date and
                    checkpoint.get('adjust') == adjust and
                    checkpoint.get('total') == len(all_stocks)):

                    last_index = checkpoint.get('last_index', 0)
                    start_index = last_index + 1
                    stats = checkpoint.get('stats', stats)
                    print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: ç¬¬ {start_index + 1} åªè‚¡ç¥¨å¼€å§‹")
                else:
                    print("âš ï¸  æ£€æŸ¥ç‚¹å‚æ•°ä¸åŒ¹é…ï¼Œä»å¤´å¼€å§‹ä¸‹è½½")
                    Path(checkpoint_path).unlink(missing_ok=True)

        # 4. éå†æ¯åªè‚¡ç¥¨
        for i in range(start_index, len(all_stocks)):
            ts_code = all_stocks[i]

            # è·³è¿‡ä¸éœ€è¦æ›´æ–°çš„è‚¡ç¥¨
            if ts_code not in incremental_dates:
                continue

            # å®šæœŸæ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯50åªè‚¡ç¥¨ï¼‰
            if (i + 1) % 50 == 1 or i == len(all_stocks) - 1:
                print(f"\n{'='*60}")
                print(f"è¿›åº¦: [{i + 1}/{stats['total']}]")
                print(f"æˆåŠŸ: {stats['success']} | å¤±è´¥: {stats['failed']} | è·³è¿‡: {stats['skipped']}")
                print(f"{'='*60}")

            try:
                # ä½¿ç”¨å¢é‡å¼€å§‹æ—¥æœŸ
                start_date = incremental_dates[ts_code]
                # è°ƒç”¨ç°æœ‰çš„ save_daily æ–¹æ³•
                self.save_daily(ts_code, start_date, end_date, adjust)
                stats['success'] += 1
            except Exception as e:
                print(f"âŒ {ts_code} å¤„ç†å¤±è´¥: {e}")
                stats['failed'] += 1

            # æ¯10åªè‚¡ç¥¨ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            if (i + 1) % 10 == 0:
                checkpoint_data = {
                    'default_start_date': default_start_date,
                    'end_date': end_date,
                    'adjust': adjust,
                    'total': len(all_stocks),
                    'last_index': i,
                    'stats': stats,
                    'timestamp': datetime.now().isoformat()
                }
                self._save_checkpoint(checkpoint_path, checkpoint_data)

        # 5. åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆä¸‹è½½å®Œæˆï¼‰
        if Path(checkpoint_path).exists():
            Path(checkpoint_path).unlink()
            print("ğŸ—‘ï¸  å·²åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶")

        # 6. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"æ•°æ®ä¸‹è½½å®Œæˆ:")
        print(f"  æ€»è®¡: {stats['total']} åªè‚¡ç¥¨")
        print(f"  æˆåŠŸ: {stats['success']}")
        print(f"  å¤±è´¥: {stats['failed']}")
        print(f"  è·³è¿‡: {stats['skipped']}")
        print(f"{'='*60}")

        return stats

    def save_all_stocks_by_date(self, start_date: str = "20240101",
                                end_date: str = None):
        """
        æŒ‰äº¤æ˜“æ—¥æ‰¹é‡è·å–å…¨éƒ¨Aè‚¡æ•°æ®ï¼ˆä¸å«å¤æƒä»·æ ¼ï¼‰

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # å¦‚æœæœªæŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
        if end_date is None:
            end_date = datetime.today().strftime("%Y%m%d")

        # 1. è·å–äº¤æ˜“æ—¥å†
        print("ğŸ“… æ­£åœ¨è·å–äº¤æ˜“æ—¥å†...")
        try:
            df_cal = self._retry_api_call(
                self.pro.trade_cal,
                exchange='SSE',
                is_open='1',
                start_date=start_date,
                end_date=end_date,
                fields='cal_date'
            )
            if df_cal is None or df_cal.empty:
                print("âŒ è·å–äº¤æ˜“æ—¥å†å¤±è´¥")
                return None
            trade_dates = df_cal['cal_date'].tolist()
            print(f"ğŸ“… å…± {len(trade_dates)} ä¸ªäº¤æ˜“æ—¥")
        except Exception as e:
            print(f"âŒ è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
            return None

        # 2. éå†æ¯ä¸ªäº¤æ˜“æ—¥
        stats = {'success': 0, 'failed': 0, 'total': len(trade_dates)}
        total_records = 0

        for i, date in enumerate(trade_dates, 1):
            # å®šæœŸæ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯20ä¸ªäº¤æ˜“æ—¥ï¼‰
            if i % 20 == 1 or i == len(trade_dates):
                print(f"\n{'='*60}")
                print(f"è¿›åº¦: [{i}/{stats['total']}]")
                print(f"æˆåŠŸ: {stats['success']} | å¤±è´¥: {stats['failed']} | æ€»è®°å½•: {total_records}")
                print(f"{'='*60}")

            # ä½¿ç”¨é‡è¯•æœºåˆ¶è·å–æ•°æ®
            df = self._retry_api_call(
                self.pro.daily,
                trade_date=date
            )

            if df is not None and not df.empty:
                # æ•°æ®è½¬æ¢å’Œä¿å­˜
                saved_count = self._save_daily_batch(df, date)
                total_records += saved_count
                stats['success'] += 1
                print(f"âœ… {date} ä¿å­˜äº† {saved_count} æ¡è®°å½•")
            else:
                stats['failed'] += 1
                print(f"âš ï¸ {date} è·å–æ•°æ®å¤±è´¥")

        # 3. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"æ•°æ®ä¸‹è½½å®Œæˆ:")
        print(f"  æ€»è®¡: {stats['total']} ä¸ªäº¤æ˜“æ—¥")
        print(f"  æˆåŠŸ: {stats['success']}")
        print(f"  å¤±è´¥: {stats['failed']}")
        print(f"  æ€»è®°å½•: {total_records} æ¡")
        print(f"{'='*60}")

        return stats

    def _save_daily_batch(self, df: pd.DataFrame, trade_date: str) -> int:
        """
        ä¿å­˜æ‰¹é‡è·å–çš„æ—¥çº¿æ•°æ®

        Args:
            df: Tushare daily æ¥å£è¿”å›çš„DataFrame
            trade_date: äº¤æ˜“æ—¥æœŸ

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        # ä¿å­˜ä¸å¤æƒä»·æ ¼
        df['open_orig'] = df['open']
        df['high_orig'] = df['high']
        df['low_orig'] = df['low']
        df['close_orig'] = df['close']
        df['open_qfq'] = None
        df['high_qfq'] = None
        df['low_qfq'] = None
        df['close_qfq'] = None
        df['turnover'] = None

        # é‡å‘½ååˆ—
        df = df.rename(columns={
            "trade_date": "datetime",
            "vol": "volume"
        })

        # æ·»åŠ å…ƒæ•°æ®
        df["symbol"] = df['ts_code'].str.split('.').str[0]
        df["exchange"] = df['ts_code'].apply(self._detect_exchange)
        df["interval"] = "1d"
        df["datetime"] = pd.to_datetime(df["datetime"]).dt.strftime("%Y-%m-%d")

        # æ·»åŠ  amount åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if 'amount' not in df.columns:
            df['amount'] = None

        # é€‰æ‹©è¦ä¿å­˜çš„åˆ—
        columns = ["symbol", "exchange", "interval", "datetime",
                  "open", "high", "low", "close",
                  "open_qfq", "high_qfq", "low_qfq", "close_qfq",
                  "pre_close", "change", "pct_chg",
                  "volume", "turnover", "amount"]

        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
        for col in columns:
            if col not in df.columns:
                df[col] = None

        # ä¿å­˜åˆ°æ•°æ®åº“
        try:
            df[columns].to_sql(
                "bars", self.engine, if_exists="append", index=False, method="multi"
            )
            return len(df)
        except Exception as e:
            if "UNIQUE constraint" in str(e) or "duplicate" in str(e).lower():
                # æ•°æ®å·²å­˜åœ¨ï¼Œè¿”å›å·²ä¿å­˜æ•°é‡
                return 0
            else:
                print(f"  âš ï¸  æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
                return 0

    # ==================== è´¢åŠ¡æ•°æ®ç›¸å…³æ–¹æ³• ====================

    def _create_financial_table_from_df(self, table_name: str, df: pd.DataFrame):
        """
        æ ¹æ®DataFrameåŠ¨æ€åˆ›å»ºè´¢åŠ¡æŠ¥è¡¨è¡¨

        Args:
            table_name: è¡¨åï¼ˆå¦‚ income_000001ï¼‰
            df: åŒ…å«åˆ—ä¿¡æ¯çš„DataFrame
        """
        if df.empty:
            raise ValueError("DataFrameä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºè¡¨")

        # æ„å»ºåˆ—å®šä¹‰
        columns_def = []
        for col in df.columns:
            if col in ['ts_code', 'ann_date', 'end_date']:
                columns_def.append(f"{col} TEXT NOT NULL")
            else:
                columns_def.append(f"{col} REAL")

        # ä¸»é”®å®šä¹‰
        primary_key = "PRIMARY KEY (ts_code, ann_date, end_date)"

        # åˆ›å»ºè¡¨SQL
        sql = f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            {', '.join(columns_def)},
            {primary_key}
        );
        """

        # æ‰§è¡Œå»ºè¡¨
        try:
            with self.engine.connect() as conn:
                conn.execute(text(sql))
                conn.commit()
            print(f"âœ… è¡¨ {table_name} å·²åˆ›å»º")
        except Exception as e:
            print(f"âš ï¸  åˆ›å»ºè¡¨ {table_name} å¤±è´¥: {e}")
            raise

    def _extract_stock_code(self, ts_code: str) -> str:
        """
        ä» ts_code ä¸­æå–çº¯è‚¡ç¥¨ä»£ç 

        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 000001.SZï¼‰

        Returns:
            çº¯è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 000001ï¼‰
        """
        return ts_code.split('.')[0]

    def save_income(self, ts_code: str, start_date: str = None, end_date: str = None):
        """
        è·å–å¹¶ä¿å­˜åˆ©æ¶¦è¡¨æ•°æ®

        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 000001.SZ æˆ– 000001ï¼‰
            start_date: å…¬å‘Šå¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ YYYYMMDDï¼‰
            end_date: å…¬å‘Šç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ YYYYMMDDï¼‰

        Returns:
            ä¿å­˜çš„è®°å½•æ•°ï¼Œå¤±è´¥è¿”å› 0
        """
        try:
            # æ ‡å‡†åŒ–ä»£ç 
            ts_code_std = self._standardize_code(ts_code)
            code = self._extract_stock_code(ts_code_std)
            table_name = f"income_{code}"

            # è·å–æ•°æ®
            print(f"  ğŸ“¥ è·å–åˆ©æ¶¦è¡¨æ•°æ® {ts_code_std}...")
            df = self._retry_api_call(
                self.pro.income,
                ts_code=ts_code_std,
                start_date=start_date,
                end_date=end_date
            )

            if df is None or df.empty:
                print(f"  âš ï¸  {ts_code_std} æ— åˆ©æ¶¦è¡¨æ•°æ®")
                return 0

            # å…ˆå¯¹APIè¿”å›çš„æ•°æ®å»é‡
            df_before = len(df)
            df = df.drop_duplicates(subset=['ts_code', 'ann_date', 'end_date'], keep='last')
            if len(df) < df_before:
                print(f"  ğŸ”„ å»é™¤é‡å¤æ•°æ®: {df_before} -> {len(df)} æ¡")

            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
            table_exists = False
            with self.engine.connect() as conn:
                result = conn.execute(text(
                    f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'"
                ))
                table_exists = result.fetchone() is not None

            # ä¿å­˜åˆ°æ•°æ®åº“
            if not table_exists:
                # è¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºå¹¶æ’å…¥
                self._create_financial_table_from_df(table_name, df)
                df.to_sql(table_name, self.engine, if_exists="append", index=False, method="multi")
                print(f"  âœ… å·²ä¿å­˜åˆ©æ¶¦è¡¨ {len(df)} æ¡è®°å½•")
            else:
                # è¡¨å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é‡å¤
                existing_df = pd.read_sql_query(f"SELECT ts_code, ann_date, end_date FROM {table_name}", self.engine)
                if existing_df.empty:
                    # è¡¨ä¸ºç©ºï¼Œç›´æ¥æ’å…¥
                    df.to_sql(table_name, self.engine, if_exists="append", index=False, method="multi")
                    print(f"  âœ… å·²ä¿å­˜åˆ©æ¶¦è¡¨ {len(df)} æ¡è®°å½•")
                else:
                    # åˆå¹¶å¹¶å»é‡
                    merged_df = pd.concat([existing_df, df], ignore_index=True)
                    merged_df = merged_df.drop_duplicates(subset=['ts_code', 'ann_date', 'end_date'], keep='last')

                    # åˆ é™¤æ—§è¡¨å¹¶é‡æ–°åˆ›å»º
                    with self.engine.connect() as conn:
                        conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
                        conn.commit()
                    self._create_financial_table_from_df(table_name, merged_df)
                    merged_df.to_sql(table_name, self.engine, if_exists="append", index=False, method="multi")
                    print(f"  âœ… å·²ä¿å­˜åˆ©æ¶¦è¡¨ {len(merged_df)} æ¡è®°å½•ï¼ˆå«å†å²æ•°æ®ï¼‰")

            return len(df)

        except Exception as e:
            error_msg = str(e)
            if "æ— æƒé™" in error_msg or "æƒé™" in error_msg or "403" in error_msg:
                print(f"  âš ï¸  æ— æƒé™è·å–åˆ©æ¶¦è¡¨æ•°æ®ï¼ˆéœ€è¦2000+ç§¯åˆ†ï¼‰")
            else:
                print(f"  âŒ ä¿å­˜åˆ©æ¶¦è¡¨å¤±è´¥: {e}")
            return 0

    def save_balancesheet(self, ts_code: str, start_date: str = None, end_date: str = None):
        """
        è·å–å¹¶ä¿å­˜èµ„äº§è´Ÿå€ºè¡¨æ•°æ®

        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 000001.SZ æˆ– 000001ï¼‰
            start_date: å…¬å‘Šå¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ YYYYMMDDï¼‰
            end_date: å…¬å‘Šç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ YYYYMMDDï¼‰

        Returns:
            ä¿å­˜çš„è®°å½•æ•°ï¼Œå¤±è´¥è¿”å› 0
        """
        try:
            # æ ‡å‡†åŒ–ä»£ç 
            ts_code_std = self._standardize_code(ts_code)
            code = self._extract_stock_code(ts_code_std)
            table_name = f"balancesheet_{code}"

            # è·å–æ•°æ®
            print(f"  ğŸ“¥ è·å–èµ„äº§è´Ÿå€ºè¡¨æ•°æ® {ts_code_std}...")
            df = self._retry_api_call(
                self.pro.balancesheet,
                ts_code=ts_code_std,
                start_date=start_date,
                end_date=end_date
            )

            if df is None or df.empty:
                print(f"  âš ï¸  {ts_code_std} æ— èµ„äº§è´Ÿå€ºè¡¨æ•°æ®")
                return 0

            # å…ˆå¯¹APIè¿”å›çš„æ•°æ®å»é‡
            df_before = len(df)
            df = df.drop_duplicates(subset=['ts_code', 'ann_date', 'end_date'], keep='last')
            if len(df) < df_before:
                print(f"  ğŸ”„ å»é™¤é‡å¤æ•°æ®: {df_before} -> {len(df)} æ¡")

            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
            table_exists = False
            with self.engine.connect() as conn:
                result = conn.execute(text(
                    f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'"
                ))
                table_exists = result.fetchone() is not None

            # ä¿å­˜åˆ°æ•°æ®åº“
            if not table_exists:
                self._create_financial_table_from_df(table_name, df)
                df.to_sql(table_name, self.engine, if_exists="append", index=False, method="multi")
                print(f"  âœ… å·²ä¿å­˜èµ„äº§è´Ÿå€ºè¡¨ {len(df)} æ¡è®°å½•")
            else:
                existing_df = pd.read_sql_query(f"SELECT ts_code, ann_date, end_date FROM {table_name}", self.engine)
                if existing_df.empty:
                    df.to_sql(table_name, self.engine, if_exists="append", index=False, method="multi")
                    print(f"  âœ… å·²ä¿å­˜èµ„äº§è´Ÿå€ºè¡¨ {len(df)} æ¡è®°å½•")
                else:
                    merged_df = pd.concat([existing_df, df], ignore_index=True)
                    merged_df = merged_df.drop_duplicates(subset=['ts_code', 'ann_date', 'end_date'], keep='last')
                    with self.engine.connect() as conn:
                        conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
                        conn.commit()
                    self._create_financial_table_from_df(table_name, merged_df)
                    merged_df.to_sql(table_name, self.engine, if_exists="append", index=False, method="multi")
                    print(f"  âœ… å·²ä¿å­˜èµ„äº§è´Ÿå€ºè¡¨ {len(merged_df)} æ¡è®°å½•ï¼ˆå«å†å²æ•°æ®ï¼‰")

            return len(df)

        except Exception as e:
            error_msg = str(e)
            if "æ— æƒé™" in error_msg or "æƒé™" in error_msg or "403" in error_msg:
                print(f"  âš ï¸  æ— æƒé™è·å–èµ„äº§è´Ÿå€ºè¡¨æ•°æ®ï¼ˆéœ€è¦2000+ç§¯åˆ†ï¼‰")
            else:
                print(f"  âŒ ä¿å­˜èµ„äº§è´Ÿå€ºè¡¨å¤±è´¥: {e}")
            return 0

    def save_cashflow(self, ts_code: str, start_date: str = None, end_date: str = None):
        """
        è·å–å¹¶ä¿å­˜ç°é‡‘æµé‡è¡¨æ•°æ®

        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 000001.SZ æˆ– 000001ï¼‰
            start_date: å…¬å‘Šå¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ YYYYMMDDï¼‰
            end_date: å…¬å‘Šç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ YYYYMMDDï¼‰

        Returns:
            ä¿å­˜çš„è®°å½•æ•°ï¼Œå¤±è´¥è¿”å› 0
        """
        try:
            # æ ‡å‡†åŒ–ä»£ç 
            ts_code_std = self._standardize_code(ts_code)
            code = self._extract_stock_code(ts_code_std)
            table_name = f"cashflow_{code}"

            # è·å–æ•°æ®
            print(f"  ğŸ“¥ è·å–ç°é‡‘æµé‡è¡¨æ•°æ® {ts_code_std}...")
            df = self._retry_api_call(
                self.pro.cashflow,
                ts_code=ts_code_std,
                start_date=start_date,
                end_date=end_date
            )

            if df is None or df.empty:
                print(f"  âš ï¸  {ts_code_std} æ— ç°é‡‘æµé‡è¡¨æ•°æ®")
                return 0

            # å…ˆå¯¹APIè¿”å›çš„æ•°æ®å»é‡
            df_before = len(df)
            df = df.drop_duplicates(subset=['ts_code', 'ann_date', 'end_date'], keep='last')
            if len(df) < df_before:
                print(f"  ğŸ”„ å»é™¤é‡å¤æ•°æ®: {df_before} -> {len(df)} æ¡")

            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
            table_exists = False
            with self.engine.connect() as conn:
                result = conn.execute(text(
                    f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'"
                ))
                table_exists = result.fetchone() is not None

            # ä¿å­˜åˆ°æ•°æ®åº“
            if not table_exists:
                self._create_financial_table_from_df(table_name, df)
                df.to_sql(table_name, self.engine, if_exists="append", index=False, method="multi")
                print(f"  âœ… å·²ä¿å­˜ç°é‡‘æµé‡è¡¨ {len(df)} æ¡è®°å½•")
            else:
                existing_df = pd.read_sql_query(f"SELECT ts_code, ann_date, end_date FROM {table_name}", self.engine)
                if existing_df.empty:
                    df.to_sql(table_name, self.engine, if_exists="append", index=False, method="multi")
                    print(f"  âœ… å·²ä¿å­˜ç°é‡‘æµé‡è¡¨ {len(df)} æ¡è®°å½•")
                else:
                    merged_df = pd.concat([existing_df, df], ignore_index=True)
                    merged_df = merged_df.drop_duplicates(subset=['ts_code', 'ann_date', 'end_date'], keep='last')
                    with self.engine.connect() as conn:
                        conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
                        conn.commit()
                    self._create_financial_table_from_df(table_name, merged_df)
                    merged_df.to_sql(table_name, self.engine, if_exists="append", index=False, method="multi")
                    print(f"  âœ… å·²ä¿å­˜ç°é‡‘æµé‡è¡¨ {len(merged_df)} æ¡è®°å½•ï¼ˆå«å†å²æ•°æ®ï¼‰")

            return len(df)

        except Exception as e:
            error_msg = str(e)
            if "æ— æƒé™" in error_msg or "æƒé™" in error_msg or "403" in error_msg:
                print(f"  âš ï¸  æ— æƒé™è·å–ç°é‡‘æµé‡è¡¨æ•°æ®ï¼ˆéœ€è¦2000+ç§¯åˆ†ï¼‰")
            else:
                print(f"  âŒ ä¿å­˜ç°é‡‘æµé‡è¡¨å¤±è´¥: {e}")
            return 0

    def save_fina_indicator(self, ts_code: str, start_date: str = None, end_date: str = None) -> int:
        """
        è·å–å¹¶ä¿å­˜è´¢åŠ¡æŒ‡æ ‡æ•°æ®

        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 000001.SZ æˆ– 000001ï¼‰
            start_date: å…¬å‘Šå¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ YYYYMMDDï¼‰
            end_date: å…¬å‘Šç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ YYYYMMDDï¼‰

        Returns:
            ä¿å­˜çš„è®°å½•æ•°ï¼Œå¤±è´¥è¿”å› 0
        """
        try:
            # æ ‡å‡†åŒ–ä»£ç 
            ts_code_std = self._standardize_code(ts_code)
            table_name = "fina_indicator"

            # æ ¸å¿ƒæŒ‡æ ‡åˆ—ï¼ˆ50ä¸ªï¼‰
            core_columns = [
                # åŸºç¡€å­—æ®µ
                'ts_code', 'ann_date', 'end_date', 'report_type',
                # ç›ˆåˆ©èƒ½åŠ› (12ä¸ªæŒ‡æ ‡)
                'eps', 'basic_eps', 'diluted_eps',
                'roe', 'roa', 'roic',
                'netprofit_margin', 'grossprofit_margin', 'operateprofit_margin',
                'core_roe', 'core_roa', 'q_eps',
                # æˆé•¿èƒ½åŠ› (10ä¸ªæŒ‡æ ‡)
                'or_yoy', 'tr_yoy', 'netprofit_yoy', 'assets_yoy',
                'ebt_yoy', 'ocf_yoy', 'roe_yoy',
                'q_or_yoy', 'q_tr_yoy', 'q_netprofit_yoy',
                # è¥è¿èƒ½åŠ› (8ä¸ªæŒ‡æ ‡)
                'assets_turn', 'ar_turn', 'inv_turn',
                'ca_turn', 'fa_turn', 'current_assets_turn',
                'equity_turn', 'op_npta',
                # å¿å€ºèƒ½åŠ› (8ä¸ªæŒ‡æ ‡)
                'current_ratio', 'quick_ratio', 'cash_ratio',
                'debt_to_assets', 'debt_to_eqt', 'equity_multiplier',
                'ebit_to_interest', 'op_to_ebit',
                # ç°é‡‘æµæŒ‡æ ‡ (7ä¸ªæŒ‡æ ‡)
                'ocfps', 'ocf_to_debt', 'ocf_to_shortdebt',
                'ocf_to_liability', 'ocf_to_interest',
                'cf_to_debt', 'free_cf',
                # æ¯è‚¡æŒ‡æ ‡ (3ä¸ªæŒ‡æ ‡)
                'bps', 'tangible_asset_to_share', 'capital_reserv_to_share'
            ]

            # è·å–æ•°æ®
            print(f"  ğŸ“¥ è·å–è´¢åŠ¡æŒ‡æ ‡æ•°æ® {ts_code_std}...")
            df = self._retry_api_call(
                self.pro.fina_indicator,
                ts_code=ts_code_std,
                start_date=start_date,
                end_date=end_date
            )

            if df is None or df.empty:
                print(f"  âš ï¸  {ts_code_std} æ— è´¢åŠ¡æŒ‡æ ‡æ•°æ®")
                return 0

            # å»é‡å¤„ç†
            df_before = len(df)
            df = df.drop_duplicates(subset=['ts_code', 'ann_date', 'end_date', 'report_type'], keep='last')
            if len(df) < df_before:
                print(f"  ğŸ”„ å»é™¤é‡å¤æ•°æ®: {df_before} -> {len(df)} æ¡")

            # é€‰æ‹©æ ¸å¿ƒæŒ‡æ ‡åˆ—ï¼ˆåªä¿ç•™å­˜åœ¨çš„åˆ—ï¼‰
            available_columns = [col for col in core_columns if col in df.columns]
            df = df[available_columns]

            # å…ˆåˆ é™¤é‡å¤æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            with self.engine.connect() as conn:
                # è·å–APIè¿”å›æ•°æ®çš„å…¬å‘Šæ—¥æœŸåˆ—è¡¨
                ann_dates = df['ann_date'].tolist()
                placeholders = ','.join([':ann_date_' + str(i) for i in range(len(ann_dates))])
                params = {'ts_code': ts_code_std}
                params.update({f'ann_date_{i}': date for i, date in enumerate(ann_dates)})

                delete_sql = f"""
                DELETE FROM fina_indicator
                WHERE ts_code = :ts_code AND ann_date IN ({placeholders})
                """
                conn.execute(text(delete_sql), params)
                conn.commit()

            # ä¿å­˜åˆ°æ•°æ®åº“
            df.to_sql(table_name, self.engine, if_exists="append", index=False, method="multi")
            print(f"  âœ… å·²ä¿å­˜è´¢åŠ¡æŒ‡æ ‡ {len(df)} æ¡è®°å½•")
            return len(df)

        except Exception as e:
            error_msg = str(e)
            # æƒé™ä¸è¶³æ—¶ä¼˜é›…é™çº§
            if "æ— æƒé™" in error_msg or "æƒé™" in error_msg or "403" in error_msg or "æƒé™ä¸è¶³" in error_msg:
                print(f"  âš ï¸  æ— æƒé™è·å–è´¢åŠ¡æŒ‡æ ‡æ•°æ®ï¼ˆéœ€è¦2000+ç§¯åˆ†ï¼‰")
            else:
                print(f"  âŒ ä¿å­˜è´¢åŠ¡æŒ‡æ ‡å¤±è´¥: {e}")
            return 0

    def check_fina_indicator_access(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰è´¢åŠ¡æŒ‡æ ‡æ¥å£è®¿é—®æƒé™

        Returns:
            True è¡¨ç¤ºæœ‰æƒé™ï¼ŒFalse è¡¨ç¤ºæ— æƒé™
        """
        try:
            test_df = self.pro.fina_indicator(ts_code='000001.SZ', limit=1)
            return test_df is not None and not test_df.empty
        except:
            return False

    def save_all_financial(self, ts_code: str, start_date: str = None, end_date: str = None, include_indicators: bool = True):
        """
        è·å–å¹¶ä¿å­˜æ‰€æœ‰è´¢åŠ¡æŠ¥è¡¨æ•°æ®ï¼ˆåˆ©æ¶¦è¡¨ã€èµ„äº§è´Ÿå€ºè¡¨ã€ç°é‡‘æµé‡è¡¨ã€è´¢åŠ¡æŒ‡æ ‡ï¼‰

        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 000001.SZ æˆ– 000001ï¼‰
            start_date: å…¬å‘Šå¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ YYYYMMDDï¼‰
            end_date: å…¬å‘Šç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ YYYYMMDDï¼‰
            include_indicators: æ˜¯å¦åŒ…å«è´¢åŠ¡æŒ‡æ ‡ï¼ˆé»˜è®¤ Trueï¼‰

        Returns:
            ä¿å­˜çš„è®°å½•æ€»æ•°
        """
        total_records = 0

        try:
            # æ ‡å‡†åŒ–ä»£ç 
            ts_code_std = self._standardize_code(ts_code)
            print(f"\n{'='*60}")
            print(f"å¼€å§‹ä¸‹è½½è´¢åŠ¡æ•°æ®: {ts_code_std}")
            print(f"{'='*60}")

            # 1. åˆ©æ¶¦è¡¨
            income_count = self.save_income(ts_code_std, start_date, end_date)
            total_records += income_count

            # 2. èµ„äº§è´Ÿå€ºè¡¨
            balance_count = self.save_balancesheet(ts_code_std, start_date, end_date)
            total_records += balance_count

            # 3. ç°é‡‘æµé‡è¡¨
            cashflow_count = self.save_cashflow(ts_code_std, start_date, end_date)
            total_records += cashflow_count

            # 4. è´¢åŠ¡æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
            indicator_count = 0
            if include_indicators:
                indicator_count = self.save_fina_indicator(ts_code_std, start_date, end_date)
                total_records += indicator_count

            print(f"\n{'='*60}")
            print(f"âœ… {ts_code_std} è´¢åŠ¡æ•°æ®ä¸‹è½½å®Œæˆ")
            print(f"  åˆ©æ¶¦è¡¨: {income_count} æ¡")
            print(f"  èµ„äº§è´Ÿå€ºè¡¨: {balance_count} æ¡")
            print(f"  ç°é‡‘æµé‡è¡¨: {cashflow_count} æ¡")
            if include_indicators:
                print(f"  è´¢åŠ¡æŒ‡æ ‡: {indicator_count} æ¡")
            print(f"  æ€»è®¡: {total_records} æ¡")
            print(f"{'='*60}")

        except Exception as e:
            print(f"âŒ {ts_code} ä¸‹è½½è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")

        return total_records

    def get_latest_financial_date(self, ts_code: str, table_type: str) -> str:
        """
        æŸ¥è¯¢æŒ‡å®šè‚¡ç¥¨çš„æœ€æ–°è´¢æŠ¥æ—¥æœŸ

        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 000001.SZ æˆ– 000001ï¼‰
            table_type: æŠ¥è¡¨ç±»å‹ï¼ˆincome/balancesheet/cashflow/fina_indicatorï¼‰

        Returns:
            æœ€æ–°å…¬å‘Šæ—¥æœŸï¼ˆæ ¼å¼ YYYYMMDDï¼‰ï¼Œæ— æ•°æ®åˆ™è¿”å› None
        """
        try:
            # æ ‡å‡†åŒ–ä»£ç 
            ts_code_std = self._standardize_code(ts_code)

            # fina_indicator ä½¿ç”¨ç»Ÿä¸€è¡¨å
            if table_type == 'fina_indicator':
                table_name = 'fina_indicator'
            else:
                code = self._extract_stock_code(ts_code_std)
                table_name = f"{table_type}_{code}"

            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            with self.engine.connect() as conn:
                result = conn.execute(text(
                    f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'"
                ))
                if not result.fetchone():
                    return None

                # æŸ¥è¯¢æœ€æ–°æ—¥æœŸ
                query = f"""
                SELECT ann_date FROM {table_name}
                WHERE ts_code = :ts_code
                ORDER BY ann_date DESC LIMIT 1
                """
                df = pd.read_sql_query(
                    query,
                    conn,
                    params={"ts_code": ts_code_std}
                )

                if not df.empty:
                    return df['ann_date'].iloc[0]
                return None

        except Exception as e:
            print(f"  âš ï¸  æŸ¥è¯¢æœ€æ–°è´¢æŠ¥æ—¥æœŸå¤±è´¥: {e}")
            return None

    # ==================== æŒ‡æ•°æ•°æ®ç›¸å…³æ–¹æ³• ====================

    def save_index_basic(self, market: str = None):
        """
        è·å–å¹¶ä¿å­˜æŒ‡æ•°åŸºæœ¬ä¿¡æ¯

        Args:
            market: å¸‚åœºä»£ç  ('SSE' ä¸Šäº¤æ‰€, 'SZSE' æ·±äº¤æ‰€)ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨

        Returns:
            ä¿å­˜çš„æŒ‡æ•°æ•°é‡ï¼ˆæ€»æ˜¯è¿”å›æ­£æ•°ï¼Œè¡¨ç¤ºæ•°æ®åº“ä¸­çš„æŒ‡æ•°æ•°é‡ï¼‰
        """
        # å…ˆè·å–æ•°æ®
        print(f"  ğŸ“¥ è·å–æŒ‡æ•°åŸºæœ¬ä¿¡æ¯ (market={market or 'å…¨éƒ¨'})...")
        df = self._retry_api_call(
            self.pro.index_basic,
            market=market or ''
        )

        if df is None or df.empty:
            print(f"  âš ï¸  æ— æŒ‡æ•°åŸºæœ¬ä¿¡æ¯")
            # å³ä½¿ API è¿”å›ç©ºï¼Œä¹Ÿæ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰æ•°æ®
            with self.engine.connect() as conn:
                query = "SELECT COUNT(*) FROM index_names"
                if market == 'SSE':
                    query += " WHERE ts_code LIKE '%.SH'"
                elif market == 'SZSE':
                    query += " WHERE ts_code LIKE '%.SZ'"
                result = conn.execute(text(query))
                count = result.fetchone()[0]
                return count

        # å‡†å¤‡æ•°æ®
        df = df.copy()
        df['updated_at'] = datetime.now().isoformat()

        # å°è¯•ä¿å­˜åˆ°æ•°æ®åº“
        try:
            df.to_sql('index_names', self.engine, if_exists='append', index=False, method='multi')
            print(f"  âœ… å·²ä¿å­˜ {len(df)} æ¡æŒ‡æ•°åŸºæœ¬ä¿¡æ¯")
            return len(df)
        except Exception as e:
            error_msg = str(e)
            if "UNIQUE constraint" in error_msg or "duplicate" in error_msg.lower():
                # æ•°æ®å·²å­˜åœ¨ï¼Œä¸éœ€è¦æ›´æ–°ï¼ˆåŸºæœ¬ä¿¡æ¯é€šå¸¸ä¸å˜ï¼‰
                # ç›´æ¥è¿”å›æ•°æ®åº“ä¸­çš„æ•°é‡
                with self.engine.connect() as conn:
                    query = "SELECT COUNT(*) FROM index_names"
                    if market == 'SSE':
                        query += " WHERE ts_code LIKE '%.SH'"
                    elif market == 'SZSE':
                        query += " WHERE ts_code LIKE '%.SZ'"
                    result = conn.execute(text(query))
                    count = result.fetchone()[0]
                print(f"  â„¹ï¸  æŒ‡æ•°åŸºæœ¬ä¿¡æ¯å·²å­˜åœ¨ï¼Œæ•°æ®åº“ä¸­å…±æœ‰ {count} æ¡")
                return count
            else:
                print(f"  âŒ ä¿å­˜æŒ‡æ•°åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
                return 0

    def save_index_daily(self, ts_code: str, start_date: str = "20200101", end_date: str = None):
        """
        ä¿å­˜æŒ‡æ•°æ—¥çº¿æ•°æ®

        Args:
            ts_code: æŒ‡æ•°ä»£ç ï¼ˆå¦‚ 000001.SHï¼‰
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDDï¼ŒNoneåˆ™ä½¿ç”¨ä»Šå¤©

        Returns:
            ä¿å­˜çš„è®°å½•æ•°ï¼Œå¤±è´¥è¿”å› 0
        """
        try:
            # å¦‚æœæœªæŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
            if end_date is None:
                end_date = datetime.today().strftime("%Y%m%d")

            # æ ‡å‡†åŒ–ä»£ç 
            if '.' not in ts_code:
                raise ValueError(f"æŒ‡æ•°ä»£ç æ ¼å¼é”™è¯¯: {ts_code}ï¼Œåº”ä¸º 000001.SH æ ¼å¼")

            # è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®
            print(f"  ğŸ“¥ è·å–æŒ‡æ•°æ—¥çº¿æ•°æ® {ts_code} ({start_date} - {end_date})...")
            df = self._retry_api_call(
                self.pro.index_daily,
                ts_code=ts_code,
                start_date=start_date,
                end_date=end_date
            )

            if df is None or df.empty:
                print(f"  âš ï¸  {ts_code} æ— æŒ‡æ•°æ—¥çº¿æ•°æ®")
                return 0

            # é‡å‘½ååˆ—ä»¥åŒ¹é… bars è¡¨ç»“æ„
            df = df.rename(columns={
                "trade_date": "datetime",
                "vol": "volume"
            })

            # æ·»åŠ å…ƒæ•°æ®
            # æŒ‡æ•°ä½¿ç”¨å®Œæ•´çš„ ts_code ä½œä¸º symbolï¼ˆå¦‚ 000001.SHï¼‰ï¼Œé¿å…ä¸è‚¡ç¥¨ä»£ç å†²çª
            df["symbol"] = ts_code
            df["exchange"] = self._detect_exchange(ts_code)
            df["interval"] = "1d"
            df["datetime"] = pd.to_datetime(df["datetime"]).dt.strftime("%Y-%m-%d")

            # æŒ‡æ•°æ•°æ®æ²¡æœ‰çš„è‚¡ç¥¨å­—æ®µï¼Œè®¾ä¸º None
            stock_only_fields = [
                'open_qfq', 'high_qfq', 'low_qfq', 'close_qfq',  # å‰å¤æƒä»·æ ¼
                'turnover',  # æ¢æ‰‹ç‡
                # ä¼°å€¼æŒ‡æ ‡
                'pe', 'pe_ttm', 'pb', 'ps', 'ps_ttm',
                # å¸‚å€¼æŒ‡æ ‡
                'total_mv', 'circ_mv',
                # è‚¡æœ¬ç»“æ„
                'total_share', 'float_share', 'free_share',
                # æµåŠ¨æ€§æŒ‡æ ‡
                'volume_ratio', 'turnover_rate_f',
                # åˆ†çº¢æŒ‡æ ‡
                'dv_ratio', 'dv_ttm'
            ]
            for field in stock_only_fields:
                df[field] = None

            # é€‰æ‹©è¦ä¿å­˜çš„åˆ—
            columns = ["symbol", "exchange", "interval", "datetime",
                      "open", "high", "low", "close",
                      "open_qfq", "high_qfq", "low_qfq", "close_qfq",
                      "pre_close", "change", "pct_chg",
                      "volume", "turnover", "amount",
                      # Daily basic æŒ‡æ ‡
                      "turnover_rate_f", "volume_ratio",
                      "pe", "pe_ttm", "pb", "ps", "ps_ttm",
                      "total_mv", "circ_mv",
                      "total_share", "float_share", "free_share",
                      "dv_ratio", "dv_ttm"]

            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
            for col in columns:
                if col not in df.columns:
                    df[col] = None

            # ä¿å­˜åˆ°æ•°æ®åº“
            df[columns].to_sql("bars", self.engine, if_exists="append", index=False, method="multi")
            print(f"  âœ… å·²ä¿å­˜ {ts_code} å…± {len(df)} æ¡è®°å½•")
            return len(df)

        except Exception as e:
            # æ•°æ®åº“æ“ä½œå¤±è´¥
            if "UNIQUE constraint" in str(e) or "duplicate" in str(e).lower():
                # æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡
                print(f"  â­ï¸  {ts_code} æŒ‡æ•°æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                return 0
            else:
                print(f"  âŒ {ts_code} æŒ‡æ•°æ•°æ®ä¿å­˜å¤±è´¥: {e}")
                return 0

    def save_all_indices(self, start_date: str = "20240101", end_date: str = None, markets: list = None):
        """
        æ‰¹é‡ä¸‹è½½æ‰€æœ‰æŒ‡æ•°æ•°æ®

        Args:
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDDï¼ŒNoneåˆ™ä½¿ç”¨ä»Šå¤©
            markets: å¸‚åœºåˆ—è¡¨ ['SSE', 'SZSE']ï¼ŒNoneåˆ™è¡¨ç¤ºå…¨éƒ¨

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # å¦‚æœæœªæŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
        if end_date is None:
            end_date = datetime.today().strftime("%Y%m%d")

        # é»˜è®¤å¸‚åœº
        if markets is None:
            markets = ['SSE', 'SZSE']

        # ç¬¬ä¸€æ­¥ï¼šè·å–æŒ‡æ•°åŸºæœ¬ä¿¡æ¯
        print("ğŸ“‹ æ­£åœ¨è·å–æŒ‡æ•°åˆ—è¡¨...")
        all_indices = []

        for market in markets:
            try:
                count = self.save_index_basic(market=market)
                if count > 0:
                    # ä»æ•°æ®åº“è¯»å–æŒ‡æ•°ä»£ç 
                    query = "SELECT ts_code FROM index_names"
                    if market == 'SSE':
                        query += " WHERE ts_code LIKE '%.SH'"
                    elif market == 'SZSE':
                        query += " WHERE ts_code LIKE '%.SZ'"

                    with self.engine.connect() as conn:
                        df = pd.read_sql_query(query, conn)
                        all_indices.extend(df['ts_code'].tolist())
            except Exception as e:
                print(f"  âŒ è·å– {market} æŒ‡æ•°åˆ—è¡¨å¤±è´¥: {e}")

        if not all_indices:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æŒ‡æ•°")
            return {'total': 0, 'success': 0, 'failed': 0}

        # å»é‡
        all_indices = list(set(all_indices))
        print(f"ğŸ“‹ å…± {len(all_indices)} ä¸ªæŒ‡æ•°")

        # ç¬¬äºŒæ­¥ï¼šé€ä¸ªä¸‹è½½æŒ‡æ•°è¡Œæƒ…æ•°æ®
        stats = {'total': len(all_indices), 'success': 0, 'failed': 0, 'skipped': 0}

        for i, ts_code in enumerate(all_indices):
            # å®šæœŸæ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 10 == 1 or i == len(all_indices) - 1:
                print(f"\n{'='*60}")
                print(f"è¿›åº¦: [{i + 1}/{stats['total']}]")
                print(f"æˆåŠŸ: {stats['success']} | å¤±è´¥: {stats['failed']} | è·³è¿‡: {stats['skipped']}")
                print(f"{'='*60}")

            try:
                result = self.save_index_daily(ts_code, start_date, end_date)
                if result > 0:
                    stats['success'] += 1
                elif result == 0:
                    stats['skipped'] += 1
                else:
                    stats['failed'] += 1
            except Exception as e:
                print(f"  âŒ {ts_code} å¤„ç†å¤±è´¥: {e}")
                stats['failed'] += 1

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"æŒ‡æ•°æ•°æ®ä¸‹è½½å®Œæˆ:")
        print(f"  æ€»è®¡: {stats['total']} ä¸ªæŒ‡æ•°")
        print(f"  æˆåŠŸ: {stats['success']}")
        print(f"  å¤±è´¥: {stats['failed']}")
        print(f"  è·³è¿‡: {stats['skipped']}")
        print(f"{'='*60}")

        return stats

    # ==================== ç”³ä¸‡è¡Œä¸šåˆ†ç±»ç›¸å…³æ–¹æ³• ====================

    def save_sw_classify(self, src: str = 'SW2021', level: str = None,
                         update_timestamp: bool = True) -> int:
        """
        è·å–å¹¶ä¿å­˜ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ•°æ®

        Args:
            src: è¡Œä¸šåˆ†ç±»æ¥æºï¼ŒSW2014=ç”³ä¸‡2014ç‰ˆæœ¬ï¼ŒSW2021=ç”³ä¸‡2021ç‰ˆæœ¬ï¼ˆé»˜è®¤ï¼‰
            level: è¡Œä¸šåˆ†çº§ï¼ŒL1=ä¸€çº§ï¼ŒL2=äºŒçº§ï¼ŒL3=ä¸‰çº§ï¼ŒNone=å…¨éƒ¨
            update_timestamp: æ˜¯å¦æ›´æ–°æ—¶é—´æˆ³ï¼ŒFalseæ—¶ä¿ç•™æ—§æ—¶é—´æˆ³ç”¨äºå¢é‡æ›´æ–°

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        try:
            print(f"  ğŸ“¥ è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ•°æ® (src={src}, level={level or 'å…¨éƒ¨'})...")

            # è·å–æ•°æ®
            params = {'src': src}
            if level:
                params['level'] = level

            df = self._retry_api_call(
                self.pro.index_classify,
                **params
            )

            if df is None or df.empty:
                print(f"  âš ï¸  æ— ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ•°æ®")
                return 0

            # å‡†å¤‡æ•°æ®
            df = df.copy()
            df['src'] = src
            df['updated_at'] = datetime.now().isoformat()

            # é€‰æ‹©åˆ—å¹¶ä¿å­˜
            columns = ['index_code', 'industry_name', 'parent_code', 'level', 'industry_code', 'is_pub', 'src', 'updated_at']

            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
            for col in columns:
                if col not in df.columns:
                    df[col] = None

            df = df[columns]

            if update_timestamp:
                # åˆ é™¤æ—§æ•°æ®å¹¶é‡æ–°æ’å…¥
                delete_sql = "DELETE FROM sw_classify WHERE src = :src"
                with self.engine.connect() as conn:
                    conn.execute(text(delete_sql), {"src": src})
                    conn.commit()

                df.to_sql('sw_classify', self.engine, if_exists='append', index=False, method='multi')
            else:
                # ä½¿ç”¨ upsert ä¿ç•™æ—§æ—¶é—´æˆ³
                upsert_sql = """
                INSERT INTO sw_classify (index_code, industry_name, parent_code, level, industry_code, is_pub, src, updated_at)
                VALUES (:index_code, :industry_name, :parent_code, :level, :industry_code, :is_pub, :src, :updated_at)
                ON CONFLICT(index_code) DO UPDATE SET
                    industry_name = :industry_name,
                    parent_code = :parent_code,
                    level = :level,
                    industry_code = :industry_code,
                    is_pub = :is_pub,
                    src = :src
                """

                with self.engine.connect() as conn:
                    for _, row in df.iterrows():
                        conn.execute(text(upsert_sql), {
                            "index_code": row['index_code'],
                            "industry_name": row['industry_name'],
                            "parent_code": row['parent_code'],
                            "level": row['level'],
                            "industry_code": row['industry_code'],
                            "is_pub": row['is_pub'],
                            "src": row['src'],
                            "updated_at": row['updated_at']
                        })
                    conn.commit()

            print(f"  âœ… å·²ä¿å­˜ç”³ä¸‡è¡Œä¸šåˆ†ç±» {len(df)} æ¡è®°å½•")
            return len(df)

        except Exception as e:
            error_msg = str(e)
            if "æ— æƒé™" in error_msg or "æƒé™" in error_msg or "403" in error_msg:
                print(f"  âš ï¸  æ— æƒé™è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ•°æ®ï¼ˆéœ€è¦2000+ç§¯åˆ†ï¼‰")
            else:
                print(f"  âŒ ä¿å­˜ç”³ä¸‡è¡Œä¸šåˆ†ç±»å¤±è´¥: {e}")
            return 0

    def save_sw_members(self, index_code: str = None, ts_code: str = None,
                       is_new: str = 'Y', force_update: bool = False) -> int:
        """
        è·å–å¹¶ä¿å­˜ç”³ä¸‡è¡Œä¸šæˆåˆ†è‚¡æ•°æ®

        Args:
            index_code: è¡Œä¸šæŒ‡æ•°ä»£ç ï¼ŒNoneè¡¨ç¤ºè·å–æ‰€æœ‰
            ts_code: è‚¡ç¥¨ä»£ç ï¼Œä¸index_codeäºŒé€‰ä¸€
            is_new: æ˜¯å¦æœ€æ–°æˆåˆ†ï¼ŒY=æ˜¯ï¼ˆé»˜è®¤ï¼‰ï¼ŒN=å¦
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°ï¼ˆåˆ é™¤æ—§æ•°æ®ï¼‰

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        try:
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            params = {'is_new': is_new}
            if index_code:
                params['index_code'] = index_code
            if ts_code:
                params['ts_code'] = ts_code

            desc = f"index_code={index_code}" if index_code else f"ts_code={ts_code}" if ts_code else "å…¨éƒ¨"
            print(f"  ğŸ“¥ è·å–ç”³ä¸‡è¡Œä¸šæˆåˆ†è‚¡æ•°æ® ({desc}, is_new={is_new})...")

            # è·å–æ•°æ®
            df = self._retry_api_call(
                self.pro.index_member_all,
                **params
            )

            if df is None or df.empty:
                print(f"  âš ï¸  æ— ç”³ä¸‡è¡Œä¸šæˆåˆ†è‚¡æ•°æ®")
                return 0

            # å¦‚æœæŒ‡å®šäº† index_codeï¼Œåˆ é™¤æ—§æ•°æ®
            if index_code and force_update:
                delete_sql = "DELETE FROM sw_members WHERE index_code = :index_code"
                with self.engine.connect() as conn:
                    conn.execute(text(delete_sql), {"index_code": index_code})
                    conn.commit()

            # å‡†å¤‡æ•°æ®
            df = df.copy()
            df['is_new'] = is_new

            # API è¿”å›çš„æ•°æ®ä¸­æ²¡æœ‰ index_code å­—æ®µï¼Œéœ€è¦æ‰‹åŠ¨æ·»åŠ 
            if index_code and 'index_code' not in df.columns:
                df['index_code'] = index_code

            # é€‰æ‹©åˆ—
            columns = ['index_code', 'ts_code', 'name', 'in_date', 'out_date', 'is_new']

            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
            for col in columns:
                if col not in df.columns:
                    df[col] = None

            df = df[columns]

            # ä¿å­˜åˆ°æ•°æ®åº“ - ä½¿ç”¨ upsert é¿å…é‡å¤æ’å…¥
            upsert_sql = """
            INSERT INTO sw_members (index_code, ts_code, name, in_date, out_date, is_new)
            VALUES (:index_code, :ts_code, :name, :in_date, :out_date, :is_new)
            ON CONFLICT(index_code, ts_code) DO UPDATE SET
                name = :name,
                in_date = :in_date,
                out_date = :out_date,
                is_new = :is_new
            """

            with self.engine.connect() as conn:
                for _, row in df.iterrows():
                    conn.execute(text(upsert_sql), {
                        "index_code": row['index_code'],
                        "ts_code": row['ts_code'],
                        "name": row['name'],
                        "in_date": row['in_date'],
                        "out_date": row['out_date'],
                        "is_new": row['is_new']
                    })
                conn.commit()

            print(f"  âœ… å·²ä¿å­˜ç”³ä¸‡è¡Œä¸šæˆåˆ†è‚¡ {len(df)} æ¡è®°å½•")
            return len(df)

        except Exception as e:
            error_msg = str(e)
            if "æ— æƒé™" in error_msg or "æƒé™" in error_msg or "403" in error_msg:
                print(f"  âš ï¸  æ— æƒé™è·å–ç”³ä¸‡è¡Œä¸šæˆåˆ†è‚¡æ•°æ®ï¼ˆéœ€è¦2000+ç§¯åˆ†ï¼‰")
            else:
                print(f"  âŒ ä¿å­˜ç”³ä¸‡è¡Œä¸šæˆåˆ†è‚¡å¤±è´¥: {e}")
            return 0

    def get_outdated_indices(self, src: str = 'SW2021', days: int = 7) -> list:
        """
        è·å–éœ€è¦æ›´æ–°çš„è¡Œä¸šä»£ç åˆ—è¡¨ï¼ˆæ ¹æ® updated_at åˆ¤æ–­ï¼‰

        Args:
            src: è¡Œä¸šåˆ†ç±»æ¥æº
            days: è¶…è¿‡å¤šå°‘å¤©æœªæ›´æ–°åˆ™éœ€è¦æ›´æ–°

        Returns:
            éœ€è¦æ›´æ–°çš„è¡Œä¸šä»£ç åˆ—è¡¨
        """
        import pandas as pd
        cutoff_date = (datetime.now() - pd.Timedelta(days=days)).isoformat()

        query = """
        SELECT index_code FROM sw_classify
        WHERE src = :src
        AND (updated_at IS NULL OR updated_at < :cutoff_date)
        """

        with self.engine.connect() as conn:
            df = pd.read_sql_query(query, conn, params={"src": src, "cutoff_date": cutoff_date})

        return df['index_code'].tolist() if not df.empty else []

    def update_indices_timestamp(self, index_codes: list, src: str = 'SW2021'):
        """
        æ›´æ–°æŒ‡å®šè¡Œä¸šçš„ updated_at æ—¶é—´æˆ³

        Args:
            index_codes: è¡Œä¸šä»£ç åˆ—è¡¨
            src: è¡Œä¸šåˆ†ç±»æ¥æº
        """
        if not index_codes:
            return

        now = datetime.now().isoformat()
        placeholders = ','.join([f':code{i}' for i in range(len(index_codes))])
        params = {f'code{i}': code for i, code in enumerate(index_codes)}
        params['src'] = src
        params['now'] = now

        update_sql = f"""
        UPDATE sw_classify
        SET updated_at = :now
        WHERE index_code IN ({placeholders})
        AND src = :src
        """

        with self.engine.connect() as conn:
            conn.execute(text(update_sql), params)
            conn.commit()

    def save_all_sw_industry(self, src: str = 'SW2021', is_new: str = 'Y',
                            force_update: bool = False, incremental: bool = False,
                            incremental_days: int = 7) -> dict:
        """
        è·å–å¹¶ä¿å­˜æ‰€æœ‰ç”³ä¸‡è¡Œä¸šåˆ†ç±»å’Œæˆåˆ†è‚¡æ•°æ®

        Args:
            src: è¡Œä¸šåˆ†ç±»æ¥æºï¼ŒSW2014=ç”³ä¸‡2014ç‰ˆæœ¬ï¼ŒSW2021=ç”³ä¸‡2021ç‰ˆæœ¬ï¼ˆé»˜è®¤ï¼‰
            is_new: æ˜¯å¦æœ€æ–°æˆåˆ†ï¼ŒY=æ˜¯ï¼ˆé»˜è®¤ï¼‰ï¼ŒN=å¦
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
            incremental: æ˜¯å¦å¢é‡æ›´æ–°ï¼ˆåªæ›´æ–°è¶…è¿‡æŒ‡å®šå¤©æ•°çš„è¡Œä¸šï¼‰
            incremental_days: å¢é‡æ›´æ–°æ—¶ï¼Œè¶…è¿‡å¤šå°‘å¤©æœªæ›´æ–°åˆ™éœ€è¦æ›´æ–°

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        print(f"\n{'='*60}")
        if incremental:
            print(f"å¼€å§‹å¢é‡æ›´æ–°ç”³ä¸‡è¡Œä¸šæ•°æ® (src={src}, days={incremental_days})")
        else:
            print(f"å¼€å§‹è·å–ç”³ä¸‡è¡Œä¸šæ•°æ® (src={src})")
        print(f"{'='*60}")

        stats = {
            'classify_count': 0,
            'members_count': 0,
            'total_indices': 0,
            'skipped_indices': 0,
            'failed_indices': []
        }

        # 1. è·å–è¡Œä¸šåˆ†ç±»ï¼ˆå¢é‡æ¨¡å¼ä¸‹ä¸æ›´æ–°æ—¶é—´æˆ³ï¼‰
        print("\n1. è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»...")
        update_ts = not incremental  # å¢é‡æ¨¡å¼ä¸‹ä¸æ›´æ–°æ—¶é—´æˆ³
        classify_count = self.save_sw_classify(src=src, update_timestamp=update_ts)
        stats['classify_count'] = classify_count

        if classify_count == 0:
            print("âŒ è·å–è¡Œä¸šåˆ†ç±»å¤±è´¥")
            return stats

        # 2. è·å–éœ€è¦æ›´æ–°çš„è¡Œä¸šä»£ç 
        if incremental:
            outdated_indices = self.get_outdated_indices(src=src, days=incremental_days)
            if not outdated_indices:
                print(f"\nâœ… æ‰€æœ‰è¡Œä¸šæ•°æ®éƒ½æ˜¯æœ€æ–°çš„ï¼ˆ{incremental_days}å¤©å†…å·²æ›´æ–°ï¼‰")
                return stats

            all_indices = outdated_indices
            print(f"\n2. å¢é‡æ›´æ–°è¡Œä¸šæˆåˆ†è‚¡ï¼ˆ{len(all_indices)}/{classify_count} ä¸ªè¡Œä¸šéœ€è¦æ›´æ–°ï¼‰...")
        else:
            query = "SELECT index_code FROM sw_classify WHERE src = :src"
            with self.engine.connect() as conn:
                df_indices = pd.read_sql_query(query, conn, params={"src": src})

            if df_indices.empty:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°è¡Œä¸šåˆ†ç±»")
                return stats

            all_indices = df_indices['index_code'].tolist()
            print(f"\n2. è·å–è¡Œä¸šæˆåˆ†è‚¡ï¼ˆå…± {len(all_indices)} ä¸ªè¡Œä¸šï¼‰...")

        stats['total_indices'] = len(all_indices)

        # 3. éå†æ¯ä¸ªè¡Œä¸šè·å–æˆåˆ†è‚¡
        updated_indices = []  # è®°å½•æˆåŠŸæ›´æ–°çš„è¡Œä¸š
        for i, index_code in enumerate(all_indices):
            # å®šæœŸæ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 20 == 1 or i == len(all_indices) - 1:
                print(f"\n{'='*60}")
                print(f"è¿›åº¦: [{i + 1}/{stats['total_indices']}]")
                print(f"æˆåŠŸ: {stats['members_count']} | å¤±è´¥: {len(stats['failed_indices'])}")
                print(f"{'='*60}")

            try:
                count = self.save_sw_members(index_code=index_code, is_new=is_new, force_update=force_update)
                if count > 0:
                    stats['members_count'] += count
                    updated_indices.append(index_code)
                else:
                    stats['failed_indices'].append(index_code)

            except Exception as e:
                print(f"  âŒ {index_code} å¤„ç†å¤±è´¥: {e}")
                stats['failed_indices'].append(index_code)

        # 4. æ›´æ–°æˆåŠŸæ›´æ–°çš„è¡Œä¸šçš„æ—¶é—´æˆ³
        if incremental and updated_indices:
            self.update_indices_timestamp(updated_indices, src=src)

        # 5. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*60}")
        if incremental:
            print(f"ç”³ä¸‡è¡Œä¸šæ•°æ®å¢é‡æ›´æ–°å®Œæˆ:")
            print(f"  æ€»è¡Œä¸šæ•°: {classify_count} ä¸ª")
            print(f"  éœ€è¦æ›´æ–°: {stats['total_indices']} ä¸ª")
            print(f"  è·³è¿‡: {classify_count - stats['total_indices']} ä¸ªï¼ˆå·²æœ€æ–°ï¼‰")
        else:
            print(f"ç”³ä¸‡è¡Œä¸šæ•°æ®è·å–å®Œæˆ:")
            print(f"  è¡Œä¸šåˆ†ç±»: {stats['classify_count']} æ¡")
        print(f"  æˆåˆ†è‚¡: {stats['members_count']} æ¡")
        if stats['failed_indices']:
            print(f"  å¤±è´¥è¡Œä¸š: {len(stats['failed_indices'])} ä¸ª")
        print(f"{'='*60}")

        return stats

    def get_sw_industry_members(self, index_code: str) -> pd.DataFrame:
        """
        ä»æ•°æ®åº“è·å–æŒ‡å®šç”³ä¸‡è¡Œä¸šçš„æˆåˆ†è‚¡

        Args:
            index_code: è¡Œä¸šæŒ‡æ•°ä»£ç 

        Returns:
            æˆåˆ†è‚¡DataFrame
        """
        query = """
        SELECT m.index_code, c.industry_name, c.level, m.ts_code, m.name, m.in_date, m.out_date, m.is_new
        FROM sw_members m
        JOIN sw_classify c ON m.index_code = c.index_code
        WHERE m.index_code = :index_code
        ORDER BY m.ts_code
        """
        return pd.read_sql_query(query, self.engine, params={"index_code": index_code})

    def get_stock_sw_industry(self, ts_code: str) -> pd.DataFrame:
        """
        ä»æ•°æ®åº“è·å–æŒ‡å®šè‚¡ç¥¨æ‰€å±çš„ç”³ä¸‡è¡Œä¸š

        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 000001.SZï¼‰

        Returns:
            è¡Œä¸šä¿¡æ¯DataFrame
        """
        query = """
        SELECT m.index_code, c.industry_name, c.level, c.parent_code, m.ts_code, m.name, m.in_date, m.out_date, m.is_new
        FROM sw_members m
        JOIN sw_classify c ON m.index_code = c.index_code
        WHERE m.ts_code = :ts_code AND m.is_new = 'Y'
        ORDER BY c.level
        """
        return pd.read_sql_query(query, self.engine, params={"ts_code": ts_code})

    def get_sw_classify(self, src: str = 'SW2021', level: str = None) -> pd.DataFrame:
        """
        ä»æ•°æ®åº“è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»

        Args:
            src: è¡Œä¸šåˆ†ç±»æ¥æº
            level: è¡Œä¸šçº§åˆ«ï¼ŒNone=å…¨éƒ¨

        Returns:
            è¡Œä¸šåˆ†ç±»DataFrame
        """
        query = "SELECT * FROM sw_classify WHERE src = :src"
        params = {"src": src}

        if level:
            query += " AND level = :level"
            params["level"] = level

        query += " ORDER BY industry_code"

        return pd.read_sql_query(query, self.engine, params=params)
